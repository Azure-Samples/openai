{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question answering using embeddings-based search\n",
    "GPT excels at answering questions, but only on topics it remembers from its training data.\n",
    "What should you do if you want GPT to answer questions about unfamiliar topics? E.g.,\n",
    "- Recent events after Sep 2021\n",
    "- Your non-public documents\n",
    "- Information from past conversations\n",
    "- etc.\n",
    "\n",
    "This notebook demonstrates a two-step Search-Ask method for enabling GPT to answer questions using a library of reference text.\n",
    "\n",
    " 1. **Search:** search your library of text for relevant text sections\n",
    " 2. **Ask:** insert the retrieved text sections into a message to GPT and ask it the question\"\n",
    "\n",
    "## Why search is better than fine-tuning\n",
    "\n",
    "GPT can learn knowledge in two ways:\n",
    "\n",
    " - Via model weights (i.e., fine-tune the model on a training set)\n",
    " - Via model inputs (i.e., insert the knowledge into an input message)\n",
    "\n",
    "Although fine-tuning can feel like the more natural option—training on data is how GPT learned all of its other knowledge, after all—we generally do not recommend it as a way to teach the model knowledge. Fine-tuning is better suited to teaching specialized tasks or styles, and is less reliable for factual recall.\n",
    "\n",
    "As an analogy, model weights are like long-term memory. When you fine-tune a model, it's like studying for an exam a week away. When the exam arrives, the model may forget details, or misremember facts it never read.\n",
    "\n",
    "In contrast, message inputs are like short-term memory. When you insert knowledge into a message, it's like taking an exam with open notes. With notes in hand, the model is more likely to arrive at correct answers.\n",
    "\n",
    "One downside of text search relative to fine-tuning is that each model is limited by a maximum amount of text it can read at once:\n",
    "\n",
    "| Model           | Maximum text length       |\n",
    "|-----------------|---------------------------|\n",
    "| `gpt-3.5-turbo` | 4,096 tokens (~5 pages)   |\n",
    "| `gpt-4`         | 8,192 tokens (~10 pages)  |\n",
    "| `gpt-4-32k`     | 32,768 tokens (~40 pages) |\n",
    "\n",
    "Continuing the analogy, you can think of the model like a student who can only look at a few pages of notes at a time, despite potentially having shelves of textbooks to draw upon.\n",
    "\n",
    "Therefore, to build a system capable of drawing upon large quantities of text to answer questions, we recommend using a Search-Ask approach.\n",
    "Continuing the analogy, you can think of the model like a student who can only look at a few pages of notes at a time, despite potentially having shelves of textbooks to draw upon.\n",
    "\n",
    "Therefore, to build a system capable of drawing upon large quantities of text to answer questions, we recommend using a Search-Ask approach.\n",
    "\n",
    "## Search\n",
    "Text can be searched in many ways. E.g.,\n",
    "- Lexical-based search\n",
    "- Graph-based search\n",
    "- Embedding-based search\n",
    "\n",
    "This example notebook uses embedding-based search. [Embeddings](https://platform.openai.com/docs/guides/embeddings) are simple to implement and work especially well with questions, as questions often don't lexically overlap with their answers.\n",
    "\n",
    "Consider embeddings-only search as a starting point for your own system. Better search systems might combine multiple search methods, along with features like popularity, recency, user history, redundancy with prior search results, click rate data, etc. Q&A retrieval performance may also be improved with techniques like [HyDE](https://arxiv.org/abs/2212.10496), in which questions are first transformed into hypothetical answers before being embedded. Similarly, GPT can also potentially improve search results by automatically transforming questions into sets of keywords or search terms.\n",
    "\n",
    "## Full procedure\n",
    "Specifically, this notebook demonstrates the following procedure:\n",
    "1. Prepare search data (once per document)\n",
    "    1. Collect: We'll download a few hundred Wikipedia articles about the 2022 Olympics\n",
    "    2. Chunk: Documents are split into short, mostly self-contained sections to be embedded\n",
    "    3. Embed: Each section is embedded with the OpenAI API\n",
    "    4. Store: Embeddings are saved (for large datasets, use a vector database)\n",
    "2. Search (once per query)\n",
    "    1. Given a user question, generate an embedding for the query from the OpenAI API\n",
    "    2. Using the embeddings, rank the text sections by relevance to the query\n",
    "3. Ask (once per query)\n",
    "    1. Insert the question and the most relevant sections into a message to GPT\n",
    "    2. Return GPT's answer\n",
    "\n",
    "### Costs\n",
    "Because GPT is more expensive than embeddings search, a system with a decent volume of queries will have its costs dominated by step 3.\n",
    "\n",
    "- For `gpt-3.5-turbo` using ~1,000 tokens per query, it costs ~$0.002 per query, or ~500 queries per dollar (as of Apr 2023)\n",
    "- For `gpt-4`, again assuming ~1,000 tokens per query, it costs ~$0.03 per query, or ~30 queries per dollar (as of Apr 2023)\n",
    "Of course, exact costs will depend on the system specifics and usage patterns.\n",
    "\n",
    "## Preamble\n",
    "We'll begin by:\n",
    "- Importing the necessary libraries\n",
    "- Selecting models for embeddings search and question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Install the Azure Open AI SDK using the below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Azure.AI.OpenAI, 1.0.0-beta.8</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: Azure.AI.OpenAI, *-*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#i \"nuget:https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-tools/nuget/v3/index.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget:Microsoft.DotNet.Interactive.AIUtilities, *-*\"\n",
    "\n",
    "using Microsoft.DotNet.Interactive;\n",
    "using Microsoft.DotNet.Interactive.AIUtilities;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var azureOpenAIKey = await Kernel.GetPasswordAsync(\"Provide your OPEN_AI_KEY\");\n",
    "\n",
    "// Your endpoint should look like the following https://YOUR_OPEN_AI_RESOURCE_NAME.openai.azure.com/\n",
    "var azureOpenAIEndpoint = await Kernel.GetInputAsync(\"Provide the OPEN_AI_ENDPOINT\");\n",
    "\n",
    "// Enter the deployment name you chose when you deployed the model.\n",
    "var deployment = await Kernel.GetInputAsync(\"Provide deployment name\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import namesapaces and create an instance of `OpenAiClient` using the `azureOpenAIEndpoint` and the `azureOpenAIKey`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using Azure;\n",
    "using Azure.AI.OpenAI;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "OpenAIClient client = new (new Uri(azureOpenAIEndpoint), new AzureKeyCredential(azureOpenAIKey.GetClearTextPassword()));"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "csharp"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
