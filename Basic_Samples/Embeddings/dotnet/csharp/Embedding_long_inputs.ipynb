{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding texts that are longer than the model's maximum context length\n",
    "OpenAI's embedding models cannot embed text that exceeds a maximum length. The maximum length varies by model, and is measured by _tokens_, not string length. If you are unfamiliar with tokenization, check out [How to count tokens with tiktoken](How_to_count_tokens_with_tiktoken.ipynb).\n",
    "\n",
    "This notebook shows how to handle texts that are longer than a model's maximum context length. We'll demonstrate using embeddings from `text-embedding-ada-002`, but the same ideas can be applied to other models and tasks. To learn more about embeddings, check out the [OpenAI Embeddings Guide](https://beta.openai.com/docs/guides/embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Install the Azure Open AI SDK using the below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Azure.AI.OpenAI, 1.0.0-beta.14</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: Azure.AI.OpenAI, 1.0.0-beta.14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget:Microsoft.DotNet.Interactive.AIUtilities, 1.0.0-beta.24129.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using Microsoft.DotNet.Interactive;\n",
    "using Microsoft.DotNet.Interactive.AIUtilities;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this cell, it will prompt you for the apiKey, endPoint, and embedding deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var azureOpenAIKey = await Kernel.GetPasswordAsync(\"Provide your OPEN_AI_KEY\");\n",
    "\n",
    "// Your endpoint should look like the following https://YOUR_OPEN_AI_RESOURCE_NAME.openai.azure.com/\n",
    "var azureOpenAIEndpoint = await Kernel.GetInputAsync(\"Provide the OPEN_AI_ENDPOINT\");\n",
    "\n",
    "// Enter the deployment name you chose when you deployed the model.\n",
    "var deployment = await Kernel.GetInputAsync(\"Provide embedding deployment name\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import namesapaces and create an instance of `OpenAiClient` using the `azureOpenAIEndpoint` and the `azureOpenAIKey`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using Azure;\n",
    "using Azure.AI.OpenAI;\n",
    "using System.Collections.Generic;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "OpenAIClient client = new (new Uri(azureOpenAIEndpoint), new AzureKeyCredential(azureOpenAIKey.GetClearTextPassword()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var longText = string.Join(\" \", Enumerable.Repeat(\"AGI\", 5000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the following cell\n",
    "\n",
    "It will display and error like:\n",
    "```\n",
    "Azure.RequestFailedException: This model's maximum context length is 8191 tokens, however you requested 10000 tokens (10000 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\n",
    "\n",
    "Status: 400 (model_error)\n",
    "\n",
    "Content:\n",
    "\n",
    "{\n",
    "\n",
    "  \"error\": {\n",
    "\n",
    "    \"message\": \"This model's maximum context length is 8191 tokens, however you requested 10000 tokens (10000 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\",\n",
    "\n",
    "    \"type\": \"invalid_request_error\",\n",
    "\n",
    "    \"param\": null,\n",
    "\n",
    "    \"code\": null\n",
    "\n",
    "  }\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "This shows that we have crossed the limit of `8191` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Azure.RequestFailedException: This model's maximum context length is 8191 tokens, however you requested 10000 tokens (10000 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\r\nStatus: 400 (model_error)\r\n\r\nContent:\r\n{\n  \"error\": {\n    \"message\": \"This model's maximum context length is 8191 tokens, however you requested 10000 tokens (10000 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\",\n    \"type\": \"invalid_request_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n\r\n\r\nHeaders:\r\nAccess-Control-Allow-Origin: REDACTED\r\nX-Content-Type-Options: REDACTED\r\napim-request-id: REDACTED\r\nX-Request-ID: REDACTED\r\nms-azureml-model-error-reason: REDACTED\r\nms-azureml-model-error-statuscode: REDACTED\r\nx-ms-client-request-id: 89940e48-7900-40f3-ba70-68eef1b6a149\r\nx-ms-region: REDACTED\r\nStrict-Transport-Security: REDACTED\r\nDate: Tue, 07 Nov 2023 12:16:57 GMT\r\nContent-Length: 294\r\nContent-Type: application/json\r\n\r\n   at Azure.Core.HttpPipelineExtensions.ProcessMessageAsync(HttpPipeline pipeline, HttpMessage message, RequestContext requestContext, CancellationToken cancellationToken)\r\n   at Azure.AI.OpenAI.OpenAIClient.GetEmbeddingsAsync(EmbeddingsOptions embeddingsOptions, CancellationToken cancellationToken)\r\n   at Submission#8.<<Initialize>>d__0.MoveNext()\r\n--- End of stack trace from previous location ---\r\n   at Microsoft.CodeAnalysis.Scripting.ScriptExecutionState.RunSubmissionsAsync[TResult](ImmutableArray`1 precedingExecutors, Func`2 currentExecutor, StrongBox`1 exceptionHolderOpt, Func`2 catchExceptionOpt, CancellationToken cancellationToken)",
     "output_type": "error",
     "traceback": [
      "Azure.RequestFailedException: This model's maximum context length is 8191 tokens, however you requested 10000 tokens (10000 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\r\n",
      "Status: 400 (model_error)\r\n",
      "\r\n",
      "Content:\r\n",
      "{\n",
      "  \"error\": {\n",
      "    \"message\": \"This model's maximum context length is 8191 tokens, however you requested 10000 tokens (10000 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      "\r\n",
      "\r\n",
      "Headers:\r\n",
      "Access-Control-Allow-Origin: REDACTED\r\n",
      "X-Content-Type-Options: REDACTED\r\n",
      "apim-request-id: REDACTED\r\n",
      "X-Request-ID: REDACTED\r\n",
      "ms-azureml-model-error-reason: REDACTED\r\n",
      "ms-azureml-model-error-statuscode: REDACTED\r\n",
      "x-ms-client-request-id: 89940e48-7900-40f3-ba70-68eef1b6a149\r\n",
      "x-ms-region: REDACTED\r\n",
      "Strict-Transport-Security: REDACTED\r\n",
      "Date: Tue, 07 Nov 2023 12:16:57 GMT\r\n",
      "Content-Length: 294\r\n",
      "Content-Type: application/json\r\n",
      "\r\n",
      "   at Azure.Core.HttpPipelineExtensions.ProcessMessageAsync(HttpPipeline pipeline, HttpMessage message, RequestContext requestContext, CancellationToken cancellationToken)\r\n",
      "   at Azure.AI.OpenAI.OpenAIClient.GetEmbeddingsAsync(EmbeddingsOptions embeddingsOptions, CancellationToken cancellationToken)\r\n",
      "   at Submission#8.<<Initialize>>d__0.MoveNext()\r\n",
      "--- End of stack trace from previous location ---\r\n",
      "   at Microsoft.CodeAnalysis.Scripting.ScriptExecutionState.RunSubmissionsAsync[TResult](ImmutableArray`1 precedingExecutors, Func`2 currentExecutor, StrongBox`1 exceptionHolderOpt, Func`2 catchExceptionOpt, CancellationToken cancellationToken)"
     ]
    }
   ],
   "source": [
    "var embeddingResponse = await client.GetEmbeddingsAsync(new EmbeddingsOptions(deployment, new []{ longText }));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we want to avoid these errors, particularly when handling programmatically with a large number of embeddings. Yet, we still might be faced with texts that are longer than the maximum context length. Below we describe and provide recipes for the main approaches to handling these longer texts: (1) simply truncating the text to the maximum allowed length, and (2) chunking the text and embedding each chunk individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Truncating the input text\n",
    "The simplest solution is to truncate the input text to the maximum allowed length. Because the context length is measured in tokens, we have to first tokenize the text before truncating it. The API accepts inputs both in the form of text or tokens, so as long as you are careful that you are using the appropriate encoding, there is no need to convert the tokens back into string form. Below is an example of such a truncation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"dni-plaintext\"><pre>19999</pre></div><style>\r\n",
       ".dni-code-hint {\r\n",
       "    font-style: italic;\r\n",
       "    overflow: hidden;\r\n",
       "    white-space: nowrap;\r\n",
       "}\r\n",
       ".dni-treeview {\r\n",
       "    white-space: nowrap;\r\n",
       "}\r\n",
       ".dni-treeview td {\r\n",
       "    vertical-align: top;\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "details.dni-treeview {\r\n",
       "    padding-left: 1em;\r\n",
       "}\r\n",
       "table td {\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "table tr { \r\n",
       "    vertical-align: top; \r\n",
       "    margin: 0em 0px;\r\n",
       "}\r\n",
       "table tr td pre \r\n",
       "{ \r\n",
       "    vertical-align: top !important; \r\n",
       "    margin: 0em 0px !important;\r\n",
       "} \r\n",
       "table th {\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"dni-plaintext\"><pre>16382</pre></div><style>\r\n",
       ".dni-code-hint {\r\n",
       "    font-style: italic;\r\n",
       "    overflow: hidden;\r\n",
       "    white-space: nowrap;\r\n",
       "}\r\n",
       ".dni-treeview {\r\n",
       "    white-space: nowrap;\r\n",
       "}\r\n",
       ".dni-treeview td {\r\n",
       "    vertical-align: top;\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "details.dni-treeview {\r\n",
       "    padding-left: 1em;\r\n",
       "}\r\n",
       "table td {\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "table tr { \r\n",
       "    vertical-align: top; \r\n",
       "    margin: 0em 0px;\r\n",
       "}\r\n",
       "table tr td pre \r\n",
       "{ \r\n",
       "    vertical-align: top !important; \r\n",
       "    margin: 0em 0px !important;\r\n",
       "} \r\n",
       "table th {\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var tokenizer = await Tokenizer.CreateAsync(TokenizerModel.ada2);\n",
    "var truncated = tokenizer.TruncateByTokenCount(longText, 8191);\n",
    "longText.Length.Display();\n",
    "truncated.Length.Display();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunking the input text\n",
    "Though truncation works, discarding potentially relevant text is a clear drawback. Another approach is to divide the input text into chunks and then embed each chunk individually. Then, we can either use the chunk embeddings separately, or combine them in some way, such as averaging (weighted by the size of each chunk).\n",
    "\n",
    "Now we define a function that encodes a string into tokens and then breaks it up into chunks.\n",
    "\n",
    "Finally, we can write a function that safely handles embedding requests, even when the input text is longer than the maximum context length, by chunking the input tokens and embedding each chunk individually. The `average` flag can be set to `True` to return the weighted average of the chunk embeddings, or `False` to simply return the unmodified list of chunk embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"dni-plaintext\"><pre>5</pre></div><style>\r\n",
       ".dni-code-hint {\r\n",
       "    font-style: italic;\r\n",
       "    overflow: hidden;\r\n",
       "    white-space: nowrap;\r\n",
       "}\r\n",
       ".dni-treeview {\r\n",
       "    white-space: nowrap;\r\n",
       "}\r\n",
       ".dni-treeview td {\r\n",
       "    vertical-align: top;\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "details.dni-treeview {\r\n",
       "    padding-left: 1em;\r\n",
       "}\r\n",
       "table td {\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "table tr { \r\n",
       "    vertical-align: top; \r\n",
       "    margin: 0em 0px;\r\n",
       "}\r\n",
       "table tr td pre \r\n",
       "{ \r\n",
       "    vertical-align: top !important; \r\n",
       "    margin: 0em 0px !important;\r\n",
       "} \r\n",
       "table th {\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var textChunks = tokenizer.ChunkByTokenCount( longText, 2000, true).ToList();\n",
    "textChunks.Count.Display();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead><tr><th><i>index</i></th><th>value</th></tr></thead><tbody><tr><td>0</td><td><details class=\"dni-treeview\"><summary><span class=\"dni-code-hint\"><code>Azure.AI.OpenAI.EmbeddingItem</code></span></summary><div><table><thead><tr></tr></thead><tbody><tr><td>Embedding</td><td><div class=\"dni-plaintext\"><pre>[ -0.016826738, 0.0069990805, -0.0065451926, -0.026996454, -0.010584136, -0.0030555197, -0.024088942, 0.0011922775, -0.0144586265, -0.027154328, 0.018589662, 0.014826999, -0.011722145, -0.011662941, 0.0026197217, 0.031680048, 0.016103148, 0.011202476, 0.00795948, -0.014445471 ... (1516 more) ]</pre></div></td></tr><tr><td>Index</td><td><div class=\"dni-plaintext\"><pre>0</pre></div></td></tr></tbody></table></div></details></td></tr><tr><td>1</td><td><details class=\"dni-treeview\"><summary><span class=\"dni-code-hint\"><code>Azure.AI.OpenAI.EmbeddingItem</code></span></summary><div><table><thead><tr></tr></thead><tbody><tr><td>Embedding</td><td><div class=\"dni-plaintext\"><pre>[ -0.016608136, 0.0035173707, -0.0018321727, -0.025159389, -0.00280755, 0.0010914538, -0.017783932, 0.0016501246, -0.01102978, -0.029742327, 0.017824017, 0.0124126775, -0.015899986, -0.013060702, -0.00056702155, 0.03257493, 0.015405616, 0.009052303, 0.0072485227, -0.013321248 ... (1516 more) ]</pre></div></td></tr><tr><td>Index</td><td><div class=\"dni-plaintext\"><pre>1</pre></div></td></tr></tbody></table></div></details></td></tr><tr><td>2</td><td><details class=\"dni-treeview\"><summary><span class=\"dni-code-hint\"><code>Azure.AI.OpenAI.EmbeddingItem</code></span></summary><div><table><thead><tr></tr></thead><tbody><tr><td>Embedding</td><td><div class=\"dni-plaintext\"><pre>[ -0.016608136, 0.0035173707, -0.0018321727, -0.025159389, -0.00280755, 0.0010914538, -0.017783932, 0.0016501246, -0.01102978, -0.029742327, 0.017824017, 0.0124126775, -0.015899986, -0.013060702, -0.00056702155, 0.03257493, 0.015405616, 0.009052303, 0.0072485227, -0.013321248 ... (1516 more) ]</pre></div></td></tr><tr><td>Index</td><td><div class=\"dni-plaintext\"><pre>2</pre></div></td></tr></tbody></table></div></details></td></tr><tr><td>3</td><td><details class=\"dni-treeview\"><summary><span class=\"dni-code-hint\"><code>Azure.AI.OpenAI.EmbeddingItem</code></span></summary><div><table><thead><tr></tr></thead><tbody><tr><td>Embedding</td><td><div class=\"dni-plaintext\"><pre>[ -0.016608136, 0.0035173707, -0.0018321727, -0.025159389, -0.00280755, 0.0010914538, -0.017783932, 0.0016501246, -0.01102978, -0.029742327, 0.017824017, 0.0124126775, -0.015899986, -0.013060702, -0.00056702155, 0.03257493, 0.015405616, 0.009052303, 0.0072485227, -0.013321248 ... (1516 more) ]</pre></div></td></tr><tr><td>Index</td><td><div class=\"dni-plaintext\"><pre>3</pre></div></td></tr></tbody></table></div></details></td></tr><tr><td>4</td><td><details class=\"dni-treeview\"><summary><span class=\"dni-code-hint\"><code>Azure.AI.OpenAI.EmbeddingItem</code></span></summary><div><table><thead><tr></tr></thead><tbody><tr><td>Embedding</td><td><div class=\"dni-plaintext\"><pre>[ -0.016608136, 0.0035173707, -0.0018321727, -0.025159389, -0.00280755, 0.0010914538, -0.017783932, 0.0016501246, -0.01102978, -0.029742327, 0.017824017, 0.0124126775, -0.015899986, -0.013060702, -0.00056702155, 0.03257493, 0.015405616, 0.009052303, 0.0072485227, -0.013321248 ... (1516 more) ]</pre></div></td></tr><tr><td>Index</td><td><div class=\"dni-plaintext\"><pre>4</pre></div></td></tr></tbody></table></div></details></td></tr></tbody></table><style>\r\n",
       ".dni-code-hint {\r\n",
       "    font-style: italic;\r\n",
       "    overflow: hidden;\r\n",
       "    white-space: nowrap;\r\n",
       "}\r\n",
       ".dni-treeview {\r\n",
       "    white-space: nowrap;\r\n",
       "}\r\n",
       ".dni-treeview td {\r\n",
       "    vertical-align: top;\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "details.dni-treeview {\r\n",
       "    padding-left: 1em;\r\n",
       "}\r\n",
       "table td {\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "table tr { \r\n",
       "    vertical-align: top; \r\n",
       "    margin: 0em 0px;\r\n",
       "}\r\n",
       "table tr td pre \r\n",
       "{ \r\n",
       "    vertical-align: top !important; \r\n",
       "    margin: 0em 0px !important;\r\n",
       "} \r\n",
       "table th {\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "foreach(var chunk in textChunks.Chunk(16)){\n",
    "    var embeddings = await client.GetEmbeddingsAsync(new EmbeddingsOptions(deployment, chunk));\n",
    "    embeddings.Value.Data.Display();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, it may make sense to split chunks on paragraph boundaries or sentence boundaries to help preserve the meaning of the text."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "csharp"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
