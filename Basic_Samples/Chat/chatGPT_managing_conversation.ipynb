{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing conversation history with the ChatGPT model\n",
    "This sample notebook demonstrates a couple of simple patterns you can use for managing the prompts and conversation history with the ChatGPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os module & the OpenAI Python library for calling the OpenAI API\n",
    "# please make sure you have installed required libraries via pip install -r requirements.txt\n",
    "import os\n",
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config values\n",
    "with open(r'config.json') as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "    \n",
    "# Setting up the deployment name\n",
    "chatgpt_model_name = config_details['CHATGPT_MODEL'] \n",
    "\n",
    "# This is set to `azure`\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai.api_base = config_details['OPENAI_API_BASE']\n",
    "\n",
    "# Currently Chat Completions API have the following versions available: 2023-03-15-preview\n",
    "openai.api_version = config_details['OPENAI_API_VERSION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Create the system message for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant.\n"
     ]
    }
   ],
   "source": [
    "base_system_message = \"You are a helpful assistant.\"\n",
    "\n",
    "system_message = f\"{base_system_message.strip()}\"\n",
    "print(system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Define helper functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to send the prompt to the ChatGPT model\n",
    "# More info : https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "def send_message(messages, model_name, max_response_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=chatgpt_model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.5,\n",
    "        max_tokens=max_response_tokens,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Defining a function to print out the conversation in a readable format\n",
    "def print_conversation(messages):\n",
    "    for message in messages:\n",
    "        print(f\"[{message['role'].upper()}]\")\n",
    "        print(message['content'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Start the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the first user message that will be sent to the model. Feel free to update this.\n",
    "user_message = \"I want to write a blog post about the impact of AI on the future of work.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of messages. role can be either \"user\" or \"assistant\" \n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"name\":\"example_user\", \"content\": user_message}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 37\n"
     ]
    }
   ],
   "source": [
    "token_count = num_tokens_from_messages(messages)\n",
    "print(f\"Token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM]\n",
      "You are a helpful assistant.\n",
      "\n",
      "[USER]\n",
      "I want to write a blog post about the impact of AI on the future of work.\n",
      "\n",
      "[ASSISTANT]\n",
      "That sounds like an interesting topic! Here are some points you could consider including in your blog post:\n",
      "\n",
      "1. The rise of automation and how it is changing the workforce\n",
      "2. The potential benefits of AI in the workplace, such as increased productivity and efficiency\n",
      "3. The potential drawbacks of AI, such as job displacement and the need for retraining\n",
      "4. The importance of ethical considerations in the development and implementation of AI in the workplace\n",
      "5. The role of humans in the future of work and how they can work alongside AI to achieve better outcomes.\n",
      "\n",
      "I hope this helps! Let me know if you need any further assistance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_response_tokens = 500\n",
    "\n",
    "response = send_message(messages, chatgpt_model_name, max_response_tokens)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "print_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Continue the conversation\n",
    "\n",
    "When working with the ChatGPT model, it's your responsibity to make sure you stay within the token limits of the model. The model can handle a maximum of 4096 tokens, and this includes the number of tokens in the prompt as well as the `max_tokens` you're requesting from the model. If you exceed these limits, the model will return an error.\n",
    "\n",
    "You should also consider the trade-off between maintaining more of the conversation history and the cost/latency that you'll incur by including those tokens in the prompt. Shorter prompts are cheaper and faster. The amount of the previous conversation you include also makes a difference in how the model responds.\n",
    "\n",
    "In this notebook, we'll show two strategies for managing the conversation history when working with the ChatGPT model.\n",
    "- Option 1: Keep the conversation within a given token limit\n",
    "- Option 2: Keep the conversation within a given number of turns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Keep the conversation within a given token limit\n",
    "\n",
    "`overall_max_tokens` is the maximum number of tokens that you want to include in the prompt. Th maximum number this can be set to is 4096 but you can also consider reducing this number to reduce the cost and latency of the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_max_tokens = 4096\n",
    "prompt_max_tokens = overall_max_tokens - max_response_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue the conversation below by editing the user_message and running the cell as many times as you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 191\n",
      "[SYSTEM]\n",
      "You are a helpful assistant.\n",
      "\n",
      "[USER]\n",
      "I want to write a blog post about the impact of AI on the future of work.\n",
      "\n",
      "[ASSISTANT]\n",
      "That sounds like an interesting topic! Here are some points you could consider including in your blog post:\n",
      "\n",
      "1. The rise of automation and how it is changing the workforce\n",
      "2. The potential benefits of AI in the workplace, such as increased productivity and efficiency\n",
      "3. The potential drawbacks of AI, such as job displacement and the need for retraining\n",
      "4. The importance of ethical considerations in the development and implementation of AI in the workplace\n",
      "5. The role of humans in the future of work and how they can work alongside AI to achieve better outcomes.\n",
      "\n",
      "I hope this helps! Let me know if you need any further assistance.\n",
      "\n",
      "[USER]\n",
      "The target audience for the blog post should be business leaders working in the tech industry.\n",
      "\n",
      "[ASSISTANT]\n",
      "Great! In that case, you may want to tailor your blog post to address the specific concerns and interests of business leaders in the tech industry. Here are some additional points you could consider including:\n",
      "\n",
      "1. The competitive advantage that AI can provide to businesses in the tech industry\n",
      "2. The potential impact of AI on business models and revenue streams\n",
      "3. The importance of investing in AI research and development to stay ahead of the competition\n",
      "4. The need for a strategic approach to integrating AI into business operations\n",
      "5. The potential for AI to improve customer experiences and drive innovation.\n",
      "\n",
      "By addressing these topics, you can provide valuable insights and guidance to business leaders in the tech industry who are looking to harness the power of AI to drive growth and success.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_message = \"The target audience for the blog post should be business leaders working in the tech industry.\"\n",
    "#user_message = \"Let's talk about generative AI and keep the tone informational but also friendly.\"\n",
    "#user_message = \"Show me a few more examples\"\n",
    "messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "token_count = num_tokens_from_messages(messages)\n",
    "print(f\"Token count: {token_count}\")\n",
    "\n",
    "# remove first message while over the token limit\n",
    "while token_count > prompt_max_tokens:\n",
    "    messages.pop(0)\n",
    "    token_count = num_tokens_from_messages(messages)\n",
    "\n",
    "response = send_message(messages, chatgpt_model_name, max_response_tokens)\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "print_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Keep the conversation within a given number of turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_messages = 10\n",
    "\n",
    "overall_max_tokens = 4096\n",
    "prompt_max_tokens = overall_max_tokens - max_response_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue the conversation below by editing the user_message and running the cell as many times as you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"Make the post about generative AI aimed at business leaders who have some knowledge of the technology.\"\n",
    "messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "while len(messages) > max_messages:\n",
    "    messages.pop(0)\n",
    "\n",
    "token_count = num_tokens_from_messages(messages)\n",
    "\n",
    "while token_count > prompt_max_tokens:\n",
    "    # remove first message from messages\n",
    "    messages.pop(0)\n",
    "    token_count = num_tokens_from_messages(messages)\n",
    "\n",
    "response = send_message(messages, chatgpt_model_name)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "# print_conversation(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc180f703c9255d3d630e6d09ed4eb3355d27845db546035ce1b410f2bfa43b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
