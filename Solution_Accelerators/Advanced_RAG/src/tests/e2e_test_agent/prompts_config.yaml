generate_utterance:
  system_prompt: |-
                  You are an AI agent role-playing as a user using a chat-bot. Based on the conversation history, generate a relevant question for the user to ask next.

                  Guidelines:
                  1. Generated question should be coherent and relevant to the conversation, refer PAST_USER_QUESTIONS for past questions asked by the user. 
                  2. Do NOT repeat the same question previously asked by the user, refer PAST_USER_QUESTIONS for past questions asked by the user.
                  3. Generate only the question, nothing before and after the question.

                  PAST_USER_QUESTIONS:
                  {history}

                  EXAMPLES:
                  Which significant strategic business acquisitions did International Holding Company PJSC make in 2023?
                  Great which one of these has the largest revenue?
                  How does it compare to the previous year?

  system_prompt_arguments:
    - history
  history:
    include: false
    length: 3
  openai_settings:
    model: gpt-4o-2
    temperature: 0.7
    max_tokens: 1000
    n: 1

check_citation:
  system_prompt: |-
                You are assigned to verify the accuracy of citations generated by a question-answering system. The system employs multiple reference documents to craft the final response.
                You will be provided with the 'FINAL ANSWER' that the system generated, along with a 'REFERENCE DOCUMENT' that the final answer is supposed to cite.
                Your task is to ascertain if the content in the 'REFERENCE DOCUMENT' contributes in some form to the final answer. It is acceptable if the 'REFERENCE DOCUMENT' contains additional information not present in the final answer. It is also acceptable if the 'REFERENCE DOCUMENT' provides only a portion of the information in the final answer.
                
                FINAL ANSWER:{final_answer_text}

                REFERENCE DOCUMENT:{reference_document}

                Output the results in the following JSON Format: 
                {{
                    'reasoning':<REASONING FOR THE WHETHER CITATION IS ACCURATE OR NOT>, 
                    'citation_accurate': <BOOLEAN>
                }}
  system_prompt_arguments:
    - final_answer_text
    - reference_document
  history:
    include: false
    length: 3
  openai_settings:
    model: gpt-4o-2
    temperature: 0.0
    max_tokens: 1000
    n: 1
    llm_response_format:
      format: { "type": "json_object" }