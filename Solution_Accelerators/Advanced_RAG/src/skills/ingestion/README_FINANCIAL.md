# Ingestion Service
<!-- TOC -->
- [Financial Document Processing Pipeline](#financial-document-processing-pipeline)
    - [Document Loader](#document-loader)
    - [Document Parser](#document-parser)
        - [Parsing Financial Documents](#parsing-financial-documents)
    - [Document Indexer and Search](#document-indexer-and-search)
- [Enhancements to the Pipeline for Financial Content](#enhancements-to-the-pipeline-for-financial-content)
- [Running Ingestion Service Locally](#running-ingestion-service-locally)
- [Bring your own data](#bring-your-own-data)
<!-- /TOC -->

A guide for ingesting financial documents for your copilot. Ingestion service performs document processing and indexing using Azure Document Intelligence, Langchain, Azure Open AI, and Azure AI Search.

This section explains the behavior of the Document Indexer component and how it indexes large financial documents into Azure AI Search - a service that lets you search large corpus of data with natural language queries.

## Financial Document Processing Pipeline

Document processing consists of three core components that are invoked in the following order:

- **Document Loader**
- **Document Parser**
- **Indexer**

### Document Loader

Document loader loads the document in-memory and extracts the text and additional metadata from them. There are two main services that can be used to achieve this:

1. `Azure Document Intelligence` service
2. `LangChain` w/Azure Document Intelligence as the document loader

Essentially, both of these loaders do the same task â€“ parse the content of the file and generate a `Document` - pieces of text and associated metadata. The difference between the two is the output format as each service provides a unique set of tools to extract further information once a parsed form of the PDF document is ready.

### Document Parser

Document parsing processes the text and metadata from document loading and splits the document into smaller chunks based on the layout. It also cleans up the text by removing newlines, additional whitespaces, and any markdown tags in the parsed output generated by the document loader.

There are two main strategies to achieve this:

1. Fixed-size chunking with overlap: This is a good strategy for linear and uniform documents, but it might break the natural boundaries and create redundant chunks.
2. Layout-based chunking: This works well for the content is hierarchical (sections and subsections) and are of varied length; however strictly following this strategy may create too large or too small chunks.

#### Parsing Financial Documents

In the case of financial documents, the strategy that generates the best results is layout-based chunking as it is able to identify document titles and section headings and parse them. The parsed content from document titles and section headings can be used as additional context during indexing and searching to get more relevant search results.
Also, the parsing algorithm uses layout geometry to identify and isolate text content in the form of paragraph lines from tabular data and replace parsed tables with a more search-friendly formatted table. This ensures that tabular content can also be leveraged by the LLMs to generate more insightful responses to user queries.

For our use case of parsing financial documents, the parser analyzes the document based on the layout of the document, and extracts the following information from the document:

- `Title` - the top-level heading in the document based on the `MARKDOWN` output.
  - `Sections` - the individual sections within the `Title`.
    - `Lines` - the lines within a `Section`.
    - `Tables` - the financial information represented in a tabular form.

### Document Indexer and Search

As part of `Document indexing` the parsed chunks produced during document parsing along with any other metadata (provided as part of idexing the document and/or additional metadata dervived during parsing, like docunent tiltes and section headings) are indexed into a preconfigured search index in Azure AI Search service. This done using the Azure AI Search SDK.

For document searching (done when user asks for a query) `Search phase`, the user query is run against the search index - which leverages vector and semantic search to retrieve chunks with high similarity scores, and then ranks them based on semantic ranking configuration to best match the context of the user query.

## Enhancements to the Pipeline for Financial Content

1. Improved Parsing:
The Ingestion services include custom code that provides a comprehensive solution for splitting documents based on markdown headers, generating embeddings, and extracting metadata. The extracted metadata is added to the search index in additional fields and used for searching. It handles content overlap, cleanup, and page-level content extraction to ensure accurate and clean splits. The class is designed to work with Azure Document Intelligence and OpenAI for content extraction and embedding generation.

The service exposes some of these as configurations at service level and some as part of the payload:

### Ingestion Service Configuration

| Configuration Key                | Description                           |
|----------------------------------|---------------------------------------|
| MARKDOWN-HEADER-SPLIT-CONFIG     | Header 1 \| Header 2 \| Header 3 # MUST BE ORDERED |
| DOCUMENT-MAX-CHUNK-SIZE          | Maximum size of each document chunk   |
| MARKDOWN-CONTENT-INCLUDE-IMAGE-CAPTIONS | Include image captions in content |

### Ingestion Service Payload

| Payload Key                      | Description                           |
|----------------------------------|---------------------------------------|
| storage_container_name           | Azure storage container where individual pages from file will be uploaded to for citation use |
| index_name                       | Name of the search index              |
| filename                         | Name of the PDF file                  |
| reported_year                    | Reported year of the financial document |
| subsidiary                       | Subsidiary / Company name                       |
| enrichment                       | Enrichment type: NONE \| TABLE_AS_LIST \| IMAGE_DESCRIPTION |

```json
file_payload = {
    "storage_container_name": "microsoft-content",
    "index_name": "search_index_name",
    "payload": {
        "filename": "pdf_file_name",
        "reported_year": "reported_year",
        "subsidiary": "subsidiary"
    },
    "enrichment": "NONE" # can be NONE \| TABLE_AS_LIST \| IMAGE_DESCRIPTION
}
```

2. Improved Indexing:
As part of the indexing you can also provide additional fields that apply to the document like company name, report year. These fields are included with every chunk from that document and can be used for filtering during search request

## Running Ingestion Service Locally
To run ingestion service locally, ensure the following:
1. Open the repo in vscode
2. Goto src/skills/ingestion/ folder and copy and paste the `.debug.env.template` file. Rename the new file as `.debug.env`
3. Update the value of `KEYVAULT-URI` field. If you want to override what is the keyvault, you can provide them in this file.
4. Ensure you have docker desktop service running locally. If not, start Docker Desktop
5. From the `RUN AND DEBUG` dropdown of VSCode, select `Ingestion Service: Launch & Attach Server`. This will run ingestion service as a web app locally
6. Once service has started successfully, you should store [microsoft financial files](../../../data/rag/data) in storage container and submit a payload for indexing. You can do this step two ways:
- a.  Upload financial files to your storage container. Then use the examples in the [ingestion_service_rag.http](../../../docs/services/ingestion_service_rag.http) to post a request and perform ingestion / indexing of your documents.
- b. Refer to [Ingestion Notebook](../../../data/rag/microsoft_data_ingestion.ipynb). Go over instructions and run cells. Make sure to update place holders with name of index and storage account information.

### Ingesting a sample finacial report
To ingest and populate search index, upload the report to the blob store and provide the name of the report in the payload. In the payload, ensure that container name already exisits.
For examples of how to invoke ingestion service, look at sample payloads in the [ingestion_service_rag.http](../../../docs/services/ingestion_service_rag.http) file

## Bring your own data
To bring your data and use this CoPilot on it, the easiest scenario would be when your indexing needs can be met with the existing fields. However if you determine more fields are required then following things needs to be considered:
1. Search Index: Does all the fields in the current design match to your needs. If not, you will need to create new search index with required fileds to index. Here are fields and their configuration for the [microsoft_financial_index.json](../../../infra/search_index/microsoft_financial_index.json)
2. Ingestion Service: Currently ingestion service expects these additional fields to be present in the index and then uses these fields to store additional information/metadata. Here is current mapping

|Field Name|Content Stored|
|----------|--------------|
|content| actual content of the chunk in plain text|
|contentVector| vectorized form of content|
|sourcePage|Page information where the chunk is coming from. Useful for producing citations|
|sourceFile|URL of the original file|
|headings|any document title and subtitles collected for that chunk|
|reportedYear|year information of that report. used for filtering|
|subsidiary|company and or its subsidiary name to which this report belongs to. used for filtering|

If additional metadata is available and could be considered useful then that needs to be added to the index, and populated during indexing which would require changing the indexing service code -  `__generate_index_documents()` method in the [markdown_text_splitter.py](../ingestion/splitters/markdown_text_splitter.py)
Once you have your updated index and are able to run ingestion service successfully, test it out using the [Search Notebook](../../../data/rag/microsoft_data_ingestion.ipynb)

Here is an example of how to invoke [ingestion service](../../../docs/services/ingestion_service_rag.http)

3. Update the solution code:
If you are thinking that the new fields will be used for additional filtering, then [rephraser prompt](../../orchestrator_rag/app/static/static_orchestrator_prompts_config.yaml) in the orchestrator needs to be updated to produce this additional filter from the user conversation context. Currently this prompt tries to produce search request like this:
```json
"search_requests": [
                          {
                              "search_query": <rephrased_query>,
                              "subsidiary_filter": <company/subsidiary/entity>,
                              "earliest_year_filter": <year>,
                              "reasoning": <reasoning>
                          },
                          {
                              "search_query": <rephrased_query>,
                              "subsidiary_filter": <company/subsidiary/entity>,
                              "earliest_year_filter": <year>,
                              "reasoning": <reasoning>
                          }
                      ]
```
4. Update the [search index template](../search/src/components/templates/rag.config.json) that the search skil is using to match the updated search index

5. Update the data models used by the Search Skill. They can be found [here](../../common/contracts/skills/search/api_models.py):

