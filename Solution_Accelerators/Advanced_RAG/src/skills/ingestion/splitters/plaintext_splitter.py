import os
import re
import uuid
from typing import Dict, List

from shapely import geometry

from common.clients.openai.openai_client import AzureOpenAIClient
from common.clients.openai.openai_embeddings_configuration import OpenAIEmbeddingsConfiguration
from common.telemetry.app_logger import AppLogger
from azure.ai.documentintelligence.models import (
    AnalyzeResult,
    ParagraphRole,
    DocumentTable,
    DocumentParagraph,
)
from splitters.models.document_elements import (
    IndexDocument,
    TableBoundingRegion,
    Section
)
from splitters.utils.table_utils import (
    is_line_in_table,
    prettify_document_table,
    get_intersecting_table_from_line,
    get_coordinates_from_polygon
)
from splitters.utils.text_utils import (
    clean_up_text,
    get_chunks_within_max_length,
    get_base_file_name,
    get_file_name_with_page
)

class PlainTextSplitter:
    def __init__(
        self,
        logger: AppLogger,
        openai_client: AzureOpenAIClient,
        embeddings_deployment_name: str,
        max_chunk_size: int,
        markdown_content_include_image_captions: bool
    ) -> None:
        self.openai_client = openai_client
        self.embeddings_deployment_name = embeddings_deployment_name
        self.logger = logger
        self.max_chunk_size = max_chunk_size
        self.markdown_content_include_image_captions = markdown_content_include_image_captions

    async def generate_splits(self, document_file_path: str, analyzed_document: AnalyzeResult, reported_year: str, subsidiary: str, task_id: str) -> List[Dict]:
        '''
        Splits the document based on the layout generated by Azure Document Intelligence service.
        Individual paragraphs are extracted on the basis of Title, Section and Content. A single
        split generated by this method may contain one or more paragraphs as long as they are part
        of the same section.

        Tables are extracted and converted into a pretty format which is search friendly.

        Returns a list of documents represented in the same form as the search index.

        In case of an OpenAI exception, the method is retried until max tries are exhausted.
        '''
        if not os.path.isfile(document_file_path):
            self.logger.error("File path is not valid. Skipping split generation.")
            return []

        # Extract all table polygons from bounding regions.
        table_polygons: Dict[int, TableBoundingRegion] = {}
        for page_number, table in enumerate(analyzed_document.tables, start=1):
            table_polygons[page_number] = self.__generate_table_with_bounding_region_from_document_table(table)

        paragraph_title_map, _, paragraph_sectionheading_map = self.__generate_document_elements_map_from_layout(analyzed_document.paragraphs)

        documents: List[IndexDocument] = []

        previous_title = ""
        previous_content: Dict[int, str] = {}
        previous_section_heading = ""
        previous_sections: List[Section] = []

        for page_number, page in enumerate(analyzed_document.pages, start=1):
            line_number = 0
            total_lines_in_page = len(page.lines)

            while line_number < total_lines_in_page:
                # Check if the current line intersects with a table
                intersecting_table_polygon = get_intersecting_table_from_line(page.lines[line_number], table_polygons, page_number)

                # If found, skip through all lines part of the table.
                if intersecting_table_polygon is not None:
                    while line_number < total_lines_in_page and is_line_in_table(page.lines[line_number], intersecting_table_polygon):
                        line_number += 1

                    # Finally, prettify table and add to final output.
                    pretty_table = prettify_document_table(intersecting_table_polygon.table)
                    if page_number not in previous_content:
                        previous_content[page_number] = ""
                    previous_content[page_number] += f'\n\n{pretty_table}\n\n'
                else:
                    # At this point, there could be following scenarios possible:
                    # 1. If there were no intersecting tables found, we can start processing the page and build
                    #    documents.
                    # 2. If there were intersecting tables found, and they were processed successfully, and
                    #    there's more lines left in the page, continue to process them and add to page output.
                    if line_number < total_lines_in_page:
                        page_content = page.lines[line_number].content
                        if page_content in paragraph_title_map:
                            # Ensure last section is added before creating the document
                            previous_sections.append(self.__generate_section(
                                heading=previous_section_heading,
                                content_by_page=previous_content,
                                page_number=page_number
                            ))

                            # Create document with previous content
                            documents.append(self.__generate_index_document(
                                previous_title,
                                previous_sections
                            ))

                            # Update previous title with current content.
                            previous_title = page_content

                            # Reset all tracking metadata.
                            previous_sections = []
                            previous_section_heading = ""
                            previous_content.clear()

                        elif page_content in paragraph_sectionheading_map:
                            # Create new section with previous content and append to sections under current title
                            previous_sections.append(self.__generate_section(
                                heading=previous_section_heading,
                                content_by_page=previous_content,
                                page_number=page_number
                            ))

                            # Update previous section with with current content.
                            previous_section_heading = page_content

                            # Reset content to start fresh with next section.
                            previous_content.clear()
                        else:
                            if page_content:
                                if page_number not in previous_content:
                                    previous_content[page_number] = ""
                                previous_content[page_number] += f'{page_content} '

                    line_number += 1

            # If this is the last table of the document, we need to add it to the list of index documents.
            if line_number == total_lines_in_page and page_number == len(analyzed_document.pages):
                # Ensure all content is dumped in a section before creating the document.
                # If there was a section being tracked, append new content to it.
                if previous_section_heading:
                    section = previous_sections[-1]
                    section.content =  ''.join(str(previous_content[page]) for page in sorted(previous_content))
                    section.page_number = page_number

                # Finally, create document with trailing content and add it to the list of index documents.
                documents.append(self.__generate_index_document(
                    previous_title,
                    previous_sections
                ))

                # Reset content before creating indexing chunks.
                previous_content.clear()

        return await self.__generate_splits_from_index_documents(document_file_path, documents, reported_year, subsidiary)


    def __generate_index_document(self, title: str, sections: List[Section]) -> IndexDocument:
        index_document = IndexDocument()
        index_document.title = clean_up_text(title)
        index_document.sections = sections

        return index_document

    def __generate_section(self, heading: str, content_by_page: Dict[int, str], page_number: int) -> IndexDocument:
        section = Section()
        section.heading = clean_up_text(heading)
        section.content = ''.join(str(content_by_page[page]) for page in sorted(content_by_page))

        # Page number on a section is the page with max content from the document.
        section.page_number = max(content_by_page, key=lambda page: len(content_by_page[page]), default=page_number)

        return section

    async def __generate_splits_from_index_documents(self, file_path: str, index_documents: List[IndexDocument], reported_year: str, subsidiary: str) -> List[Dict]:
        splits: List[Dict] = []
        for document in index_documents:
            for section in document.sections:
                for content in get_chunks_within_max_length(section.content, self.max_chunk_size):
                    # Clean up content if cleanup enabled
                    # If cleanup is enabled and content is all markdown, ignore and go to next section.
                    content = clean_up_text(content, include_image_descriptions=self.markdown_content_include_image_captions)
                    if not content:
                        continue

                    headings = []
                    headings.append(document.title if document.title else None)
                    headings.append(section.heading if section.heading else None)

                    splits.append({
                        "id": f"{get_base_file_name(file_path)}-{uuid.uuid4()}",
                        "content": content,
                        "contentVector": await self.openai_client.create_embedding_async(
                            openai_configs=[
                                OpenAIEmbeddingsConfiguration(
                                    content=content,
                                    embeddings_deployment_name=self.embeddings_deployment_name
                                )
                            ]
                        ),
                        "headings": [heading for heading in headings if heading is not None],
                        "section": section.heading if section.heading else "N/A",
                        "sectionVector": await self.openai_client.create_embedding_async(
                            openai_configs=[
                                OpenAIEmbeddingsConfiguration(
                                    content=[section.heading] if section.heading else ["N/A"],
                                    embeddings_deployment_name=self.embeddings_deployment_name,
                                )
                            ]
                        ),
                        "sourcePage": get_file_name_with_page(file_path, section.page_number),
                        "sourceFile": get_base_file_name(file_path, True),
                        "reportedYear": reported_year,
                        "subsidiary": subsidiary
                    })

        self.logger.info(f"Successfully generated splits. Total count: {len(splits)}")
        return splits

    def __generate_table_with_bounding_region_from_document_table(self, table: DocumentTable) -> TableBoundingRegion:
        table_with_bounding_region = TableBoundingRegion()
        table_with_bounding_region.table = table

        # Extract table geometry as a polygon.
        table_with_bounding_region.polygon = geometry.Polygon([
            (point.x, point.y)
            for point in get_coordinates_from_polygon(table.bounding_regions[0].polygon)]
        )
        table_with_bounding_region.page_number = table.bounding_regions[0].page_number

        return table_with_bounding_region

    def __generate_document_elements_map_from_layout(self, paragraphs: List[DocumentParagraph]):
        paragraph_title_map = {}
        paragraph_page_header = {}
        paragraph_sectionheading_map = {}

        # Paragraphs contain information on individual role associated with the paragraph.
        # Once the roles are extracted, they can be used to fragment the document and generate
        # splits semantically.
        for paragraph in paragraphs:
            if paragraph.role == ParagraphRole.TITLE:
                paragraph_title_map[paragraph.content] = None
            elif paragraph.role == ParagraphRole.PAGE_HEADER:
                page_header = re.search(r'<!-- PageHeader="(.*?)" -->', paragraph.content)
                if page_header:
                    paragraph_page_header[page_header.group(1)] = None
            elif paragraph.role == ParagraphRole.SECTION_HEADING:
                paragraph_sectionheading_map[paragraph.content] = None

        return paragraph_title_map, paragraph_page_header, paragraph_sectionheading_map