{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "To create the training data for building a finetuning operation of a RAG, one has to consider 3 key ingredients\n",
    "\n",
    "1. Answers to finetune upon\n",
    "2. Context to support the answers\n",
    "3. Question pointing to the answer and context\n",
    "\n",
    "The reason we note the 3 in reverse to the what conventionally should happen is that for most of the time, we get the questions-answers pairs from data provider as a test data but the context is not always available. A lot of time, we can only get questions-answers pairs and it is up to finetuning team to fill in the context. In this notebook, we will demonstrate a method to fill in the context and avoid issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import jsonlines\n",
    "import os\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elements of training data\n",
    "\n",
    "All the finetuning dataset must follow the structure below\n",
    "\n",
    "`base_case =  [{\"role\": \"system\", \"content\": \"\"}, {\"role\": \"user\", \"content\": \"\"}, {\"role\": \"assistant\", \"content\": \"\"}]`\n",
    "\n",
    "The role of the finetuner is to fill in all the content according the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_case =  [{\"role\": \"system\", \"content\": \"\"}, {\"role\": \"user\", \"content\": \"\"}, {\"role\": \"assistant\", \"content\": \"\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System content\n",
    "\n",
    "In our case, the system content is where we put in our context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_content ='''\n",
    "                You are  an AI assistant. Your role includes providing data-driven insights across several focus areas:\n",
    "                Company's ESG Initiatives and Performance: Examine and report on  ESG efforts, detailing specific achievements in 2022, investments in sustainability, and adherence to the 22 ESG topics.\n",
    "                Financial Analysis: Provide comparative financial performance analyses, including revenue, profit, investments, and expenses, across different periods. Identify factors influencing financial trends, new income streams, cost control measures, and profitability improvement strategies.\n",
    "                Investment and Strategic Planning: Assess company-wide investments, significant expenses, and major investments. Evaluate their alignment with the company's strategic goals and contribution to future growth.\n",
    "                Compliance and Regulatory Oversight: Discuss compliance and regulatory issues and findings from recent audits, including measures taken to address these.\n",
    "\n",
    "                Your analysis should follow applicable laws and ethical guidelines, focusing only on information directly related to the company's strategic interests. Your goal is to aid in informed decision-making through data-driven insights and analysis.\n",
    "\n",
    "                Instructions:\n",
    "                1. Use information only from the DOCUMENTATION section and previous conversations to respond.\n",
    "                2. The DOCUMENTATION section includes search results. Each search result has two components - the document name followed by double pipe (||) and then the actual content. Always include the source (document name with extension) from which the content was used to generate the answer.\n",
    "                3. Reference the source using curly brackets. Do not combine sources; list each source separately.\n",
    "                4. Avoid repeating previously stated information or sentences.\n",
    "                5. Keep your answer relevant to the context provided. Do not infer causation or correlation, and do not divert from the topic.\n",
    "                6. Ensure that your answer can be fact-checked against the given context, so Always include source information in proper format.\n",
    "                7. If your response requires a table, create it in HTML format.\n",
    "                8. Keep your response concise and no need to explain the math behind financial calculations.\n",
    "\n",
    "                DOCUMENTATION:\n",
    "                {context}\n",
    "                '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Questions\n",
    "\n",
    "Lets suppose that we have the following question from our test dataset. \"By how much did the group's revenue increase from 2022 to 2023?\"\n",
    "\n",
    "Since this question is already in the dataset, it is best NOT to use this question directly as it reduces the number of data we can use for testing. Rather, we should consider a easier question.  \"By how much did the group's revenue increase from 2021 to 2022?\" This question has the same distribution but is not the question that is specifically being asked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = \"By how much did the group's revenue increase from 2022 to 2023?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editing the answer\n",
    "\n",
    "We get the answers from our RAG and we can check the correctness of the answer ourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Context = \"...\"\n",
    "Answer = \"The group revenue is ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Chat GPT to do editing of the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"\"\n",
    "api_key = \"\" \n",
    "\n",
    "openai.azure_endpoint = endpoint  # your endpoint should look like \"https://<your-resource-name>.openai.azure.com/\"  \n",
    "openai.api_version = \"\"  # specify the API version you're using  \n",
    "openai.api_key = api_key  \n",
    "openai.api_type = \"azure\"  \n",
    "\n",
    "def call_llm(query):\n",
    "\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[        \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        #response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "        \n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def get_llm_response(question, answer, prompt = None):\n",
    "\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\"\n",
    "    #INSTRUCTION\n",
    "    You are a Financial Expert and your task is to rephrase an answer from a financial question to make the tone sounds like those written by a financial expert.\n",
    "    Add a table of at the end of your rephrased answers to present any value and do not remove any calculation steps.\n",
    "    Keep the citation in the answer. \n",
    "\n",
    "    #INPUT\n",
    "    QUESTION: {question}\n",
    "    ANSWER: {answer}\n",
    "\n",
    "    #OUTPUT \n",
    "    provide your rephrase as a text. \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = call_llm(prompt.format(question=question, answer=answer))\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling LLM: {e}\")\n",
    "        response = None            \n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Editted_Answer = get_llm_response(Question, Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_jsonlines(Context, Question, Answer):\n",
    "    case = copy.deepcopy(base_case)\n",
    "    case[0][\"content\"] = system_content.format(context = Context)\n",
    "    case[1][\"content\"] = Question\n",
    "    case[2][\"content\"] = Answer \n",
    "    return {\"messages\": case}\n",
    "\n",
    "training_data_loc = r'training.jsonl'\n",
    "with jsonlines.open(training_data_loc, mode='w') as writer:\n",
    "    writer.write_all(build_jsonlines(Context, Question, Editted_Answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
